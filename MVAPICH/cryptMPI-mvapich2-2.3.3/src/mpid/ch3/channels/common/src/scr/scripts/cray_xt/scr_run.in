#!/bin/bash

# requires: aprun

# if SCR is disabled, just do a normal aprun and exit
if [ "$SCR_ENABLE" == "0" ] ; then
  aprun "$@"
  exit $?
fi

# turn on verbosity
if [ -n "$SCR_DEBUG" ]; then
  if [ $SCR_DEBUG -gt 0 ] ; then
    set -x
  fi
fi

prog=scr_run

libdir="@libdir@"
bindir="@bindir@"

# make a record of start time
timestamp=`date`
echo "$prog: Started: $timestamp"

# check that we have runtime dependencies
$bindir/scr_test_runtime
if [ $? -ne 0 ] ; then
  echo "$prog: exit code: 1"
  exit 1
fi

# TODO: if not in job allocation, bail out

jobid=`$bindir/scr_env --jobid`

# TODO: check that we have a valid jobid and bail if not

# get the nodeset of this job
if [ -z "$SCR_NODELIST" ] ; then
  nodelist=`$bindir/scr_env --nodes`
  if [ $? -eq 0 ] ; then
    SCR_NODELIST=$nodelist
  fi
fi
if [ -z "$SCR_NODELIST" ] ; then
  echo "$prog: ERROR: Could not identify nodeset"
  exit 1
fi
export SCR_NODELIST

# if SCR_PREFIX is not set, use the current working directory
if [ -z "$SCR_PREFIX" ] ; then
  SCR_PREFIX=`pwd`
fi
# if SCR_PREFIX is still not set, bail out
if [ -z "$SCR_PREFIX" ] ; then
  echo "$prog: ERROR: Must set \$SCR_PREFIX to prefix directory which contains checkpoint directories."
  exit 1
fi
prefix=$SCR_PREFIX

use_scr_watchdog=0
if [ "$SCR_WATCHDOG" == "1" ] ; then
  use_scr_watchdog=1
fi

# normally we would check that this script is running on a node in the job's 
# allocated nodeset, but on crays, this script runs on MOM node
script_node=`hostname`
#intersection=`$bindir/scr_glob_hosts -i $script_node:$SCR_NODELIST`
#if [ -z "$intersection" ] ; then
  #echo "$prog: ERROR: scr_run is executing on $script_node, which is not part of the job's nodeset $SCR_NODELIST."
  #exit 1
#fi

# get the control directory
cntldir=`$bindir/scr_cntl_dir`
if [ $? -ne 0 ] ; then
  echo "$prog: ERROR: Invalid control directory $cntldir."
  exit 1
fi

# NOP aprun to force every node to run prolog to delete files from cache
# TODO: remove this if admins find a better place to clear cache
aprun /bin/hostname > /dev/null

# make a record of time prerun is started
timestamp=`date`
echo "$prog: prerun: $timestamp"

# prepare checkpoint cache
$bindir/scr_prerun -p $prefix
if [ $? -ne 0 ] ; then
  echo "$prog: ERROR: Command failed: scr_prerun -p $prefix"
  exit 1
fi


# start background scr_transfer processes (1 per node) if async flush is enabled
if [ "$SCR_FLUSH_ASYNC" == "1" ] ; then
  nnodes=`$bindir/scr_glob_hosts -c -h $SCR_NODELIST`
  # original srun command to emulate
  #srun -W 0 -n${nnodes} -N${nnodes} $bindir/scr_transfer $cntldir/transfer.scrinfo &
  # aprun doesn't appear to have an equivalent to -W  (wait indefinitely after rank 
  # 0 terminates)
  aprun -n${nnodes} -N 1 $bindir/scr_transfer $cntldir/transfer.scrinfo &
fi

# start script to periodically query the time remaining in the job and write the 
# value to the halt file
# get its PID so we can kill it at exit time
$bindir/scr_check_time_remaining &
check_time_pid=$!

# enter the run loop
down_nodes=""
attempts=0
runs=${SCR_RETRIES:-0}
runs=$(($runs + 1))
runs=${SCR_RUNS:-$runs}
while [ 1 ] ; do
  # once we mark a node as bad, leave it as bad (even if it comes back healthy)
  # TODO: This hacks around the problem of accidentally deleting a checkpoint set during distribute
  #       when a relaunch lands on previously down nodes, which are healthy again.
  #       A better way would be to remember the last set used, or to provide a utility to run on *all*
  #       nodes to distribute files (also useful for defragging the machine) -- for now this works.
  keep_down=""
  if [ "$down_nodes" != "" ] ; then
    keep_down="--down $down_nodes"
  fi

  # if this is our first run, check that the free space on the drive meets requirement
  # (make sure data from job of previous user was cleaned up ok)
  # otherwise, we'll just check the total capacity
  free_flag=""
  if [ $attempts -eq 0 ] ; then
    free_flag="--free"
  fi

  # are there enough nodes to continue?
  exclude=""
  down_nodes=`$bindir/scr_list_down_nodes $free_flag $keep_down`
  if [ "$down_nodes" != "" ] ; then
    # print the reason for the down nodes, and log them
    $bindir/scr_list_down_nodes $free_flag $keep_down --log --reason --secs 0

    # if this is the first run, we hit down nodes right off the bat, make a record of them
    if [ $attempts -eq 0 ] ; then
      start_secs=`date +%s`
      echo "SCR: Failed node detected: JOBID=$jobid ATTEMPT=$attempts TIME=$start_secs NNODES=-1 RUNTIME=0 FAILED=$down_nodes"
    fi

    # determine how many nodes are needed:
    #   if SCR_MIN_NODES is set, use that
    #   otherwise, use value in nodes file if one exists
    #   otherwise, assume we need all nodes in the allocation
    # to start, assume we need all nodes in the allocation
    num_needed=`$bindir/scr_glob_hosts -c -h $SCR_NODELIST`
    if [ -n "$SCR_MIN_NODES" ]  ; then
      # if SCR_MIN_NODES is set, use that
      num_needed=$SCR_MIN_NODES
    else
      # try to lookup the number of nodes used in the last run
      num_needed_env=`$bindir/scr_env --prefix $prefix --runnodes`
      if [ $? -eq 0 ] ; then
        if [ $num_needed_env -gt 0 ] ; then
          # if the command worked, and the number is something larger than 0, go with that
          num_needed=$num_needed_env
        fi
      fi
    fi

    # check that we have enough nodes left to run the job after excluding all down nodes
    num_left=`$bindir/scr_glob_hosts -c -m $SCR_NODELIST:$down_nodes`
    if [ $num_left -lt $num_needed ] ; then
      echo "$prog: (Nodes remaining=$num_left) < (Nodes needed=$num_needed), ending run."
      break
    fi

    # check that we don't exclude the node this script is running on
    intersection=`$bindir/scr_glob_hosts -i $script_node:$down_nodes`
    if [ -n "$intersection" ] ; then
      echo "$prog: Script node $script_node is in the exclude set on retry $down_nodes, ending run."
      break
    fi

    # all checks pass, exclude the down nodes and continue
    exclude="--exclude $down_nodes"
  fi

  # make a record of when each aprun is started
  attempts=$(($attempts + 1))
  timestamp=`date`
  echo "$prog: RUN $attempts: $timestamp"

  # launch the job, make sure we include the script node and exclude down nodes
  start_secs=`date +%s`
  $bindir/scr_log_event -T "RUN STARTED" -N "Job=$jobid, Run=$attempts" -S $start_secs
  if [ -n "$SCR_CHECKPOINT_PATTERN" ] ; then export LD_PRELOAD="$libdir/libscr_interpose.so" ; fi
 

  #original srun command to emulate
  #srun --nodelist $script_node $exclude "$@"
  # aprun doesn't appear to have an equivalent to --exclude
  # on cray_xt, batch script doesn't run on the same node as rank 0 (service node)
  upnodes=""
  if [ -n "$down_nodes" ]; then
     upnodes=`$bindir/scr_glob_hosts -m $SCR_NODELIST:$down_nodes`
  else
     upnodes=$SCR_NODELIST
  fi
  # strip off the brackets around the node names because aprun doesn't like them
  tmpupnodes=${upnodes%]}
  tmpupnodes=${tmpupnodes:1}
  
  if [ $use_scr_watchdog -eq 0 ]; then
     aprun -L $tmpupnodes "$@"
  else
    echo "$prog: Attempting to start watchdog process."
    # need to get apid of the aprun command
     aprun -L $tmpupnodes "$@" &
     aprun_pid=$!;
     sleep 10; # sleep a bit to wait for the job to show up in apstat
     echo "$bindir/scr_get_jobstep_id $aprun_pid";
     apid=`$bindir/scr_get_jobstep_id $aprun_pid`;
     # then start the watchdog  if we got a valid apid
     if [ $apid != "-1" ]; then
         $bindir/scr_watchdog --dir $prefix --jobStepId $apid &
         watchdog_pid=$!
         echo "$prog: Started watchdog process with PID $watchdog_pid."
     else
        echo "$prog: ERROR: Unable to start scr_watchdog because couldn't get apid of jobstep."
        watchdog_pid=-1;
     fi
     wait $aprun_pid;
  fi

  if [ -n "$SCR_CHECKPOINT_PATTERN" ] ; then unset LD_PRELOAD ; fi
  end_secs=`date +%s`
  run_secs=$(($end_secs - $start_secs))

  # check for and log any down nodes
  $bindir/scr_list_down_nodes $keep_down --log --reason --secs $run_secs

  # log stats on the latest run attempt
  $bindir/scr_log_event -T "RUN ENDED" -N "Job=$jobid, Run=$attempts" -S $end_secs -D $run_secs

  # any retry attempts left?
  if [ $runs -gt -1 ] ; then
    runs=$(($runs - 1))
    if [ $runs -le 0 ] ; then
      echo "$prog: \$SCR_RUNS exhausted, ending run."
      break
    fi
  fi

  # TODO: this assumes aprun is running where rank 0 runs
  # is there a halt condition instructing us to stop?
  $bindir/scr_retries_halt --dir $prefix;
  if [ $? == 0 ] ; then
    echo "$prog: Halt condition detected, ending run."
    break
  fi

  # give nodes a chance to clean up
  sleep 60

  # check for halt condition again after sleep
  $bindir/scr_retries_halt --dir $prefix;
  if [ $? == 0 ] ; then
    echo "$prog: Halt condition detected, ending run."
    break
  fi
done

# stop scr_transfer processes before we attempt to scavenge 
if [ "$SCR_FLUSH_ASYNC" == "1" ] ; then
  $bindir/scr_halt --immediate
fi

# make a record of time postaprun is started
timestamp=`date`
echo "$prog: postrun: $timestamp"

# scavenge files from cache to parallel file system
$bindir/scr_postrun -p $prefix
if [ $? -ne 0 ] ; then
  echo "$prog: ERROR: Command failed: scr_postrun -p $prefix"
fi

# kill the check time remaining program
echo "$prog: kill -n KILL $check_time_pid";
kill -n KILL $check_time_pid;

# kill the watchdog process if it is running
if [ $use_scr_watchdog -eq 1 ] && [ $watchdog_pid -ne -1 ]; then
  echo "$kill -n KILL $watchdog_pid";
  kill -n KILL $watchdog_pid;
fi

# make a record of end time
timestamp=`date`
echo "$prog: Ended: $timestamp"
