#!/bin/bash

# Run this script after the final run in a job allocation
# to scavenge files from cache to parallel file system.

# if SCR is disabled, immediately exit
if [ "$SCR_ENABLE" == "0" ] ; then
  exit 0
fi

# if SCR_DEBUG is set > 0, turn on verbosity
verbose=""
if [ -n "$SCR_DEBUG" ]; then
  if [ $SCR_DEBUG -gt 0 ] ; then
    set -x
    verbose="--verbose"
  fi
fi

# record the start time for timing purposes
start_time=`date`
start_secs=`date +%s`

bindir="@bindir@"

prog="scr_postrun"

print_usage() { echo "Usage: $prog [-p prefix_dir]"; exit 1; }

# pass prefix via command line
pardir=${SCR_PREFIX:-`pwd`}
OPTIND=1
while getopts "p:" flag ; do
  case $flag in
    p) pardir=$OPTARG;;  # parallel file system prefix
    *) print_usage;;
  esac
done

# check that we have the parallel file system prefix
if [ "$pardir" == "" ] ; then
  print_usage
fi

# all parameters checked out, start normal output
echo "$prog: Started: $start_time"

# get our nodeset for this job
if [ -z "$SCR_NODELIST" ] ; then
  nodelist_env=`$bindir/scr_env --nodes`
  if [ $? -eq 0 ] ; then
    SCR_NODELIST=$nodelist_env
  fi
fi
if [ -z "$SCR_NODELIST" ] ; then
  echo "$prog: ERROR: Could not identify nodeset"
  exit 1
fi
export SCR_NODELIST

# identify what nodes are still up
UPNODES=$SCR_NODELIST
DOWNNODES=`$bindir/scr_list_down_nodes $SCR_NODELIST`
if [ "$DOWNNODES" ]; then
  UPNODES=`$bindir/scr_glob_hosts -m $SCR_NODELIST:$DOWNNODES`
fi
echo "$prog: UPNODES:   $UPNODES"

# if there is at least one remaining up node, attempt to scavenge
ret=1
if [ "$UPNODES" != "" ] ; then
  # get the SCR control directory
  cntldir=`$bindir/scr_cntl_dir`;
  # TODO: check that we have a control directory

  # TODODEST: need to scavenge latest checkpoint and any output set

  # check whether we have a dataset set to flush
  echo "$prog: Looking for latest dataset set id"
  latestid=`$bindir/scr_flush_file --dir $pardir --latest`
  if [ $? -eq 0 ] ; then
    # we have a dataset, check whether it still needs to be flushed 
    $bindir/scr_flush_file --dir $pardir --needflush $latestid
    if [ $? -eq 0 ] ; then
      # get list of possible datasets
      dataset_list=`$bindir/scr_inspect --up $UPNODES --from $cntldir`
      if [ $? -eq 0 ] ; then
        for d in $dataset_list ; do
          # determine whether this dataset needs to be flushed
          $bindir/scr_flush_file --dir $pardir --needflush $d
          if [ $? -eq 0 ] ; then
            echo "$prog: Attempting to scavenge dataset $d"

            # get directory to store files for this dataset
            subdir=`$bindir/scr_flush_file --dir $pardir --subdir $d`
            if [ $? -eq 0 ] ; then
              # build full path to dataset directory
              datadir=$pardir/$subdir
              mkdir -p $datadir

              # create directory for scr files
              mkdir -p $datadir/.scr

              # Gather files from cache to parallel file system
              echo "$prog: Scavenging files from cache to $datadir"
              echo $prog: $bindir/scr_scavenge $verbose --id $d --from $cntldir --to $datadir --jobset $SCR_NODELIST --up $UPNODES
              $bindir/scr_scavenge $verbose --id $d --from $cntldir --to $datadir --jobset $SCR_NODELIST --up $UPNODES
              echo "$prog: Done scavenging files from cache to $datadir"

              # check that gathered set is complete,
              # if not, don't update current symlink
              update_current=1
              echo "$prog: Checking that dataset is complete"
              echo "$bindir/scr_index --prefix $pardir --add $subdir"
              $bindir/scr_index --prefix $pardir --add $subdir
              if [ $? -ne 0 ] ; then
                # incomplete dataset, don't update current symlink
                update_current=0
              fi

              # if the set is complete, update the current symlink
              if [ "$update_current" == "1" ] ; then
                # make the new current
                echo "$prog: Updating current marker in index to $subdir"
                $bindir/scr_index --prefix $pardir --current $subdir

                # just completed scavenging this dataset, so quit
                ret=0
                break;
              fi
            else
              # got a dataset to flush, but failed to build flush dir name
              echo "$prog: Failed to create directory for dataset $d"
            fi
          else
            # found a dataset that has already been flushed, we can quit
            echo "$prog: Dataset $d has already been flushed"
            ret=0
            break;
          fi
        done
      else
        echo "$prog: Failed to inspect cache or cannot scavenge any datasets"
      fi
    else
      echo "$prog: Latest dataset set has already been flushed"
      ret=0
    fi
  else
    echo "$prog: Found no dataset set to scavenge"
  fi
fi

# print the timing info
end_time=`date`
end_secs=`date +%s`
run_secs=$(($end_secs - $start_secs))
echo "$prog: Ended: $end_time"
echo "$prog: secs: $run_secs"

# print the exit code and exit
echo "$prog: exit code: $ret"
exit $ret
